# -*- coding: utf-8 -*-
""""Predicting Box Office Success : A Data Driven Analysis of Film Revenue Using Machine Learning" ML Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lq1IHS3jNR9bEBpF6DkQjLvvG_9tMubu

Title of ML Project : "Predicting Box Office Success : A Data Driven Analysis of Film Revenue Using Machine Learning"

---

Name : Chrissannah Mecanzie KJ

Organization : Entri Elevate

Date : 20/11/2024

---

Step 1 : Overview of the Problem Statement

The prime motive of this project is to predict the box office revenue of films using machine learning techniques by evaluating historical data, that includes variables such as budget, cast, genre and the release timing. The model helps to understand the pattern and the main factors that might influence a film's financial success. The challenge is in accurately forecasting revenue. This analysis will provide valuable insights for film producers and studios helping them make informed about the marketing strategies to maximize profitability.

Step 2 : Objective

The objective of this project is to develop a machine learning model to accurately predict a film's box office revenue based on key factors such as budget, cast, genre, and release timing.

Step 3 : Data Description

Source -- data.world https://data.world/cye/update-movie-imdb

Features -- [Title,Budget,Genre,Director,Cast,Release date,Runtime,Production Company,Box office gross,IMDb rating,Metascore,Number of Screens,Country of origin,language,Marketing spend,Pre-release social media activity,Awards,Competition,Sequel or Original,Production Time]

Step 4 : Data Collection

---

-- Importing essential python libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
import time
import joblib

"""-- Importing the dataset"""

from google.colab import files
upload =files.upload()

df=pd.read_csv('movie_metadata (1) - movie_metadata (2).csv')

"""-- Insights"""

df.info()

df.describe()

df.head()

df.tail()

"""**Step 5 : Data Preprocessing - Data Cleaning**"""

df.isnull().sum()

df = df.dropna()
print(df.isnull().sum())

df.duplicated().sum()

df = df.drop_duplicates()

"""-- Applying imputation technique for categorical data - Mode"""

knn_imputer = KNNImputer(n_neighbors=5)
df[['budget','gross', 'duration','num_critic_for_reviews','title_year','director_facebook_likes','actor_3_facebook_likes','aspect_ratio']] = knn_imputer.fit_transform(df[['budget', 'gross', 'duration','num_critic_for_reviews','title_year','director_facebook_likes','actor_3_facebook_likes','aspect_ratio']])

df['color'].fillna(df['color'].mode()[0],inplace=True)
df['color'].fillna('Unknown',inplace=True)
df['actor_2_name'].fillna('Unknown',inplace=True)
df['actor_3_name'].fillna('Unknown',inplace=True)
df['actor_1_name'].fillna('Unknown',inplace=True)
df['plot_keywords'].fillna('Unknown',inplace=True)
df['language'].fillna('Unknown',inplace=True)
df['director_name'].fillna('Unknown',inplace=True)
df['country'].fillna('Unknown',inplace=True)
df['genres'].fillna('Unknown',inplace=True)
df['movie_title'].fillna('Unknown',inplace=True)

df['log_gross'] = np.log1p(df['gross'])
df['log_budget'] = np.log1p(df['budget'])

# Define categorical and numerical features
categorical_features = ['genres', 'content_rating', 'country']
numeric_features = ['duration', 'num_critic_for_reviews', 'log_budget', 'num_user_for_reviews', 'movie_facebook_likes']

# One-hot encode categorical features and scale numeric features
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
])

categorical_train = df.select_dtypes(include=['object']).columns
categorical_train

df[categorical_train]

imputer_categorical = SimpleImputer(strategy='most_frequent')
df[categorical_train] = imputer_categorical.fit_transform(df[categorical_train])

df.isnull().sum()

"""-- Applying imputation techniques for numerical data - Median"""

numerical = df.select_dtypes(include=['number']).columns
numerical

imputer_numerical = SimpleImputer(strategy='median')
df[numerical] = imputer_numerical.fit_transform(df[numerical])

df.isnull().sum()

numerical= df.select_dtypes(include=['number']).columns
numerical

"""-- Checking and handling Outliers"""

df[numerical].skew()

df[numerical]

"""-- Identifying Outliers with IQR Method"""

Q1 = df[numerical].quantile(0.25)
Q3 = df[numerical].quantile(0.75)

IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (df[numerical] < lower_bound) | (df[numerical] > upper_bound)

df_cleaned = df[~outliers.any(axis=1)]

(df_cleaned)

"""-- Rechecking skeweness"""

df_cleaned[numerical].skew()

"""Step 6 : Exploratory Data Analysis

-- Bar Plot : Compare counts of different categories
"""

plt.figure(figsize=(20, 8))
sns.barplot(x='content_rating', y='gross', data=df)
plt.title('Gross Revenue by Content Rating')
plt.show()

plt.figure(figsize=(20, 8))
sns.barplot(x='budget', y='content_rating', data=df)
plt.title('Budget by Content Rating')
plt.show()

plt.figure(figsize=(20, 8))
sns.barplot(x='gross', y='budget', data=df)
plt.title('Total gross earnings by budget')
plt.show()

"""-- Line plot : Visualize trends over time"""

df.groupby('title_year')['gross'].sum().plot(kind='line', title='Gross Revenue by Year')

"""Step 7 : Feature Engineering"""

df_cleaned[categorical_train]

"""-- One-Hot Encoding"""

X = df.drop(columns=['gross'])
categorical_cols = X.select_dtypes(include=['object']).columns
print("Categorical columns:", categorical_cols)
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
print(X_encoded.head())

categorical_cols

df_cleaned

"""-- Step 8 : Feature Selection

"""

features = df[['director_name', 'actor_1_name', 'budget',
                'num_critic_for_reviews', 'gross',
                'movie_facebook_likes', 'genres',
                'imdb_score','cast_total_facebook_likes','content_rating']].copy()

target = df['gross']

min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

X= df.drop(columns=['gross'])
categorical_cols = X.select_dtypes(include=['object']).columns
print("Categorical columns:", categorical_cols)
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
print(X_encoded.head())

"""Step 9 : Splitting data into training and testing sets"""

X = df.drop(columns=['gross'])
y = df['gross']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set shape: X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Testing set shape: X_test: {X_test.shape}, y_test: {y_test.shape}")

"""Step 10 : Feature Scaling"""

df.fillna(df.median(numeric_only=True), inplace=True)
df.fillna(df.mode().iloc[0], inplace=True)

X = df.drop(columns=['gross'])
y = df['gross']

min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_min_max = min_max_scaler.fit_transform(X)
X_test_min_max = min_max_scaler.transform(X_test)

X_train_standardized = standard_scaler.fit_transform(X_train)
X_test_standardized = standard_scaler.transform(X_test)

print("Min-Max Scaled Training Features:", X_train_min_max.shape)
print("Min-Max Scaled Testing Features:", X_test_min_max.shape)
print("Standardized Training Features:", X_train_standardized.shape)
print("Standardized Testing Features:", X_test_standardized.shape)

"""Step 11 : Build the ML models and Step 12 : Model Evaluation

"""

models = {
    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42),
    'Linear Regression': LinearRegression(),
    'MLP Regressor': MLPRegressor(max_iter=1000, random_state=42),
    'AdaBoost Regressor': AdaBoostRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

# Train and evaluate models
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)

    # Store results in a dictionary
    metrics = {
        'Model': name,
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'R2 Score': r2
    }
    results.append(metrics)

# Create a DataFrame from the results
results_df = pd.DataFrame(results)

# Sort the DataFrame by R2 Score in descending order
results_df_sorted = results_df.sort_values(by='R2 Score', ascending=False)

# Print the sorted DataFrame
print(results_df_sorted)

"""Step 12 : Hyperparameter Tuning"""

model = RandomForestRegressor(random_state=42)
param_dist = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,
                                   n_iter=10, scoring='neg_mean_squared_error',
                                   cv=3, verbose=2, n_jobs=-1, random_state=42)

random_search.fit(X_train, y_train)

best_params = random_search.best_params_
print(f"Best Hyperparameters: {best_params}")

best_model = random_search.best_estimator_


y_pred = best_model.predict(X_test)
print(f"R² Score: {r2_score(y_test, y_pred):.2f}")

"""-- Training and testing accuracy"""

results = []
for name, model in models.items():
    # Fit the model
    model.fit(X_train, y_train)

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Metrics
    train_r2 = r2_score(y_train, y_train_pred)  # R² on training data
    test_r2 = r2_score(y_test, y_test_pred)    # R² on testing data

    metrics = {
        'Model': name,
        'Train R²': train_r2,
        'Test R²': test_r2
    }
    results.append(metrics)

results_df = pd.DataFrame(results)
print(results_df)

"""Step 14 : Save the model"""

joblib.dump(best_model, 'gradient_boosting_model.pkl')
print("Model saved successfully.")

"""Step 15 : Test with Unseen Data"""

from google.colab import files
upload =files.upload()

unseen_df=pd.read_csv('insurance.csv')

unseen_df.fillna(unseen_df.median(numeric_only=True), inplace=True)

categorical_cols = unseen_df.select_dtypes(include=['object']).columns
unseen_df_encoded = pd.get_dummies(unseen_df, columns=categorical_cols, drop_first=True)

model = joblib.load('gradient_boosting_model.pkl')

trained_model_columns = model.feature_names_in_

for col in trained_model_columns:
    if col not in unseen_df_encoded.columns:
        unseen_df_encoded[col] = 0

unseen_df_encoded = unseen_df_encoded[trained_model_columns]

predictions = model.predict(unseen_df_encoded)

print(predictions)

"""Step 16 :Interpretation of Results (Conclusion)

-- Model Performance
The Gradient Boosting Regressor achieves near-perfect R² scores by sequentially improving predictions while preventing overfitting through regularization. and its ideal for accurate movie gross revenue predictions.

--Challenges Faced

Computational Time - Hyperparameter tuning took a considerable amount of time due to the size of the dataset and high dimentionality after encoding.

--Conclusion

In this project, multiple machine learning models were implemented to predict the gross revenue of films using various features from the dataset. After evaluating different models, Random Forest Regressordemonstrated the best performance.Despite these difficulties, the results indicate that with further data refinement and model tuning, accurate predictions of movie gross revenue can be achieved.
"""